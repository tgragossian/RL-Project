# League of Legends Jungle RL Project

Reinforcement Learning agent for League of Legends jungling, focusing on macro decisions and optimal pathing.

## ğŸ¯ Project Goal

Train an RL agent to make high-level jungling decisions:
- Optimal jungle pathing and camp clearing
- Gank timing based on lane states
- Objective control (Dragon, Herald, Baron)
- Gold/XP efficiency optimization

**Target Performance**: Gold-Plat level decision making

## ğŸ—ï¸ Project Status

Currently in **Phase 1: Simulation & Data Collection**

- âœ… Core jungle simulation (camp clearing, HP/damage)
- âœ… Monster scaling system
- âœ… Map geometry and travel times
- âœ… Lane state and gank heuristics
- ğŸš§ Data collection pipeline
- â³ RL training environment
- â³ Behavior cloning from high-elo data
- â³ PPO/DQN fine-tuning

## ğŸ“ Project Structure

```
RL-Project/
â”œâ”€â”€ src/                    # Core simulation code
â”‚   â”œâ”€â”€ combatState.py      # Camp clear simulation
â”‚   â”œâ”€â”€ monster_scaling.py  # Dynamic monster stats
â”‚   â”œâ”€â”€ map_geometry.py     # Travel time calculations
â”‚   â”œâ”€â”€ gameStates.py       # Lane states, monster spawns
â”‚   â”œâ”€â”€ jungle_env.py       # Environment integration
â”‚   â”œâ”€â”€ envSim.py           # Demo simulation
â”‚   â””â”€â”€ RiotAPIs.py         # Riot Data Dragon API client
â”œâ”€â”€ scripts/                # Utility scripts
â”‚   â””â”€â”€ test_riot_api.py    # API data exploration
â”œâ”€â”€ docs/                   # Documentation
â”‚   â””â”€â”€ data_collection_options.md  # Data strategy analysis
â”œâ”€â”€ data/                   # Training data (gitignored)
â”‚   â”œâ”€â”€ raw/                # Raw match timelines
â”‚   â””â”€â”€ processed/          # Preprocessed training data
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md              # This file
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.10+
- League of Legends understanding (jungling knowledge helpful)

### Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd RL-Project

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Running the Simulation

```bash
# Test camp clearing mechanics
python src/jungle_env.py

# Test environment simulation
python src/envSim.py
```

## ğŸ§ª Current Simulation Features

### Combat System
- HP/damage modeling
- Smite mechanics (damage + heal)
- Camp-specific stats (HP, armor, DPS)
- Death tracking for both jungler and camps

### Monster Scaling
- Level-based stat interpolation
- 8 jungle camps (Blue, Red, Gromp, Wolves, Raptors, Krugs, Scuttle)
- 4 epic objectives (Dragon, Herald, Baron, Void Grubs)
- Accurate respawn timers

### Map Geometry
- Travel time calculations between camps
- Shortest path finding (Dijkstra)
- Recall + travel time simulation
- Movement speed scaling

### Game State
- Lane state abstraction (pushed, even, under tower)
- Win probability for gank opportunities
- Heuristic gank policy (baseline)
- Epic monster spawn/respawn tracking

## ğŸ“Š Data Collection Strategy

See [docs/data_collection_options.md](docs/data_collection_options.md) for detailed analysis.

**Current Plan**: Hybrid approach
1. **Phase 1**: Riot API + inference heuristics for camp clears
2. **Phase 2**: (If needed) Computer vision on replay recordings for perfect data

## ğŸ› ï¸ Tech Stack

- **Simulation**: Python, NumPy
- **RL Framework**: Stable-Baselines3 (PPO/DQN)
- **Neural Networks**: PyTorch
- **Data Source**: Riot Games API
- **Training**: AWS (planned)

## ğŸ“ˆ Roadmap

### Phase 1: Data Collection (Current)
- [ ] Set up Riot API data pipeline
- [ ] Download 1000+ high-elo match timelines
- [ ] Build camp inference heuristics
- [ ] Extract state-action pairs

### Phase 2: Environment Setup
- [ ] Create Gymnasium-compatible environment
- [ ] Define state space (~1500 dimensions)
- [ ] Define action space (18 discrete actions)
- [ ] Implement reward function

### Phase 3: Behavior Cloning
- [ ] Train PyTorch network on expert data
- [ ] Validate imitation accuracy
- [ ] Establish baseline performance

### Phase 4: RL Fine-Tuning
- [ ] PPO training on top of behavior cloning
- [ ] Hyperparameter tuning
- [ ] Evaluation against heuristics

### Phase 5: Deployment
- [ ] AWS training pipeline
- [ ] Model checkpointing
- [ ] Performance analysis

## ğŸ¤ Contributing

This is a personal research project, but feedback and suggestions are welcome!

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- Riot Games for Data Dragon API
- League of Legends community for game knowledge
- Stable-Baselines3 team for RL framework

## ğŸ“§ Contact

[Your contact info here]

---

**Note**: This project is for educational/research purposes. It does not interact with live League of Legends games or servers.
